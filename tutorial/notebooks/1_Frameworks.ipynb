{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Frameworks - An Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1. Choosing a framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wide variety of frameworks for deep learning available:\n",
    "\n",
    "-   [TensorFlow](https://tensorflow.org) (comprehensive, widely used)\n",
    "-   [PyTorch](https://pytorch.org/) (very flexible, good for recurrent\n",
    "    networks)\n",
    "-   [Caffe](http://caffe.berkeleyvision.org/) (well-established,\n",
    "    comprehensive) and [Caffe2](https://caffe2.ai/)\n",
    "-   [Flux](https://fluxml.ai/) (generic package for differentiable\n",
    "    computing)\n",
    "-   [MatConvNet](http://www.vlfeat.org/matconvnet/), [Matlab Neural\n",
    "    Network\n",
    "    Toolbox](https://www.mathworks.com/products/neural-network.html)\n",
    "    (for Matlab users)\n",
    "-   [Microsoft Cognitive\n",
    "    Toolkit](https://www.microsoft.com/en-us/cognitive-toolkit/) (for\n",
    "    Windows users)\n",
    "-   [Theano](http://www.deeplearning.net/software/theano/),\n",
    "    [MXNet](https://mxnet.apache.org/), [Chainer](https://chainer.org/),\n",
    "    [PaddlePaddle](http://www.paddlepaddle.org/), many others"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Some common trends"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Emergence of common frontends (e.g., [Keras](https://keras.io))\n",
    "-   Emergence of exchange formats (e.g., [ONNX](https://onnx.ai/))\n",
    "-   Move towards dynamic computation, eager execution, tight Python\n",
    "    integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The main contenders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|               | TensorFlow                | PyTorch                      |\n",
    "|---------------|:--------------------------|:-----------------------------|\n",
    "| open source   | yes                       | yes                          |\n",
    "| backed by     | Google Brain              | Facebook                     |\n",
    "| user base     | large, including industry | growing, mainly research     |\n",
    "| visualization | tensorboard               | tensorboard plugin           |\n",
    "| deployment    | TensorFlow Serving        | some support via TorchScript |\n",
    "| advantages    | large feature set         | good Python integration      |\n",
    "|               | usage-specific interfaces | good for dynamic networks    |\n",
    "\n",
    "-   Recently, both frameworks have grown more similar to each other\n",
    "    -   TensorFlow includes dynamic graphs & eager execution\n",
    "    -   PyTorch includes graph compilation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2. A quick tour of TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Main frontend for TensorFlow is Python\n",
    "-   Import and use it as a Python module, similar to `numpy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### TensorFlow as a calculator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Most basic level: **tensors** (multidimensional arrays) and\n",
    "    **operations** on them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.random.normal(shape=(2,2))\n",
    "print(\"x:\", x)\n",
    "\n",
    "y = tf.linspace(0.0, 1.0, 4)\n",
    "print(\"y:\", y.numpy())\n",
    "\n",
    "y = tf.reshape(y, shape=(2,2))\n",
    "xy = tf.matmul(x, y)\n",
    "print(\"x * y (matrix op):\", xy.numpy().squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "-   Results are available immediately (**eager execution**)\n",
    "-   In the background, computational **graph** is built and run (on GPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Compiling a computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Eager execution: run tensor operations on device\n",
    "-   Compiled graph: run everything on device, including glue code\n",
    "-   To compile, annotate with `@tf.function`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function1(x, y):\n",
    "    z = tf.matmul(x, y)\n",
    "    for i in range(100):\n",
    "        z += tf.matmul(x, y)\n",
    "    return(z)\n",
    "\n",
    "@tf.function\n",
    "def function2(x, y):\n",
    "    z = tf.matmul(x, y)\n",
    "    for i in range(100):\n",
    "        z += tf.matmul(x, y)\n",
    "    return(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "x = tf.random.normal(shape=(100, 1000))\n",
    "y = tf.random.normal(shape=(1000, 500))\n",
    "\n",
    "%timeit -n 5 -r 10 function1(x, y)\n",
    "%timeit -n 5 -r 10 function2(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Grouping code in modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Core building block for new code: `tf.Module`\n",
    "    -   roughly corresponds to a node in the computational graph\n",
    "    -   provides tools for managing graph (names, variables, submodules)\n",
    "-   New code should subclass `tf.Module`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressor(tf.Module):\n",
    "    def __init__(self, input_size, output_size, name=None):\n",
    "        super(LinearRegressor, self).__init__(name=name)\n",
    "        self.w = tf.Variable(tf.random.normal([input_size, output_size]), name='w')\n",
    "        self.b = tf.Variable(tf.zeros([output_size]), name='b')\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        y = tf.matmul(x, self.w) + self.b\n",
    "        return(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# instantiate regressor with 5 inputs and 1 output\n",
    "r = LinearRegressor(5, 1)\n",
    "# apply the regressor to a batch of 10 inputs, each of size 5\n",
    "r(tf.random.normal([10, 5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Workflow:\n",
    "\n",
    "-   experiment in **eager mode**, put together computation\n",
    "-   group code in `tf.Module`s\n",
    "-   encapsulate in `@tf.function`s to compute efficiently\n",
    "-   use high-level interfaces (e.g., **Keras**) if possible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Automatic differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typical (supervised) deep learning approach:\n",
    "\n",
    "-   Model as function with parameters $\\hat{y} = f_{model}(x | \\theta)$\n",
    "    -   optimize parameters $\\theta$ to minimise loss $L(\\hat{y})$\n",
    "    -   find derivative $\\frac{dL}{d\\theta}$\n",
    "-   To find derivatives, TensorFlow offers automatic differentiation\n",
    "    with `tf.GradientTape`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create some dummy data\n",
    "x = tf.random.normal([1, 5])\n",
    "y = tf.constant([1.0])\n",
    "print(\"Initial prediction:\", r(x).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# calculate gradients of regression\n",
    "with tf.GradientTape() as t:\n",
    "    y_hat = r(x)\n",
    "    loss = tf.square(y - y_hat)\n",
    "dw, db = t.gradient(loss, [r.w, r.b])\n",
    "\n",
    "# apply gradients to improve regressor\n",
    "r.w.assign_sub(0.1 * dw)\n",
    "r.b.assign_sub(0.1 * db)\n",
    "print(\"Prediction after training:\", r(x).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exercise: Build a small neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Implement a small neural network in TensorFlow.\n",
    "-   Write a `tf.Module` named `Layer` that implements the computation of\n",
    "    a neural network layer.\n",
    "-   Write a `tf.Module` named `Network` that stacks several layers and\n",
    "    executes them in order.\n",
    "-   Train it using a `tf.GradientTape` on the regression data given\n",
    "    below (predict `y` from `x`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.reshape(tf.linspace(0., 10., 20), (1, 20))\n",
    "y = 5 - 0.5 * x + tf.random.normal(shape=(20,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Each layer should have two sets of trainable parameters: weights `w` and\n",
    "biases `b`. Given an input vector `x`, it should compute a matrix\n",
    "multiplication `w * x` and add `b` to the result. It should then apply a\n",
    "non-linearity (use `tf.nn.sigmoid()`). The output of each layer should\n",
    "be used as input for the next. The last layer should not have a\n",
    "non-linearity (or use `tf.identity()`).\n",
    "\n",
    "For both classes, `Layer` and `Network`, implement the appropriate\n",
    "`__init__()` and `__call__()` functions.\n",
    "\n",
    "As a loss function for the training, you should use the mean squared\n",
    "error between the network’s prediction and the ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### High-level interfaces: Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   [Keras](https://keras.io) - a common frontend for several frameworks\n",
    "    -   common neural network layers included\n",
    "    -   simple model construction\n",
    "    -   wrappers for training, evaluation, prediction, etc.\n",
    "-   In TensorFlow: `tf.keras`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Building a model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Input, Flatten, Dense\n",
    "\n",
    "net = Sequential([\n",
    "    Input([28, 28, 1]),\n",
    "    Flatten(),\n",
    "    Dense(1024, activation=\"elu\"),\n",
    "    Dense(1024, activation=\"elu\"),\n",
    "    Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "net.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Getting data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import mnist_imgs, mnist_lbls\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "plt.imshow(mnist_imgs[0, :, :, 0], cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Compiling and training the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "    loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "net.fit(mnist_imgs, mnist_lbls, batch_size=64, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Save and load trained models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.save('trained_model.h5')\n",
    "reloaded = tf.keras.models.load_model('trained_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "p1 = net.predict(mnist_imgs)\n",
    "p2 = reloaded.predict(mnist_imgs)\n",
    "(p1 == p2).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exercise: Convolutional networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   On image data such as MNIST, convolutional networks tend to perform\n",
    "    better than dense networks like the one above.\n",
    "-   Implement\n",
    "    [LeNet-5](http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf), one\n",
    "    of the standard architectures for MNIST."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "LeNet5 comprises the following layers:\n",
    "\n",
    "-   normalizing the input (you may do this outside of your network)\n",
    "-   convolution with 6 filters of size 5-by-5 (output of size 28x28)\n",
    "-   max-pooling with size 2 and stride 2\n",
    "-   convolution with 16 filters of size 5-by-5\n",
    "    -   the original paper only conneced some of the input features to\n",
    "        each output map, ignore this for now\n",
    "    -   here, we use valid convolution, such that the output size is\n",
    "        10-by-10\n",
    "-   max-pooling with size 2 and stride 2\n",
    "-   a convolution layer with 120 features and kernel size 5-by-5, again\n",
    "    using valid convolution (i.e., getting a 1-by-1 output)\n",
    "-   a fully connected layer with 84 units\n",
    "-   the output layer with 10 units\n",
    "\n",
    "Diverging from the original formulation of the network a little, use\n",
    "ReLU non-linearities and a softmax cross-entropy loss. LeNet5 achieves\n",
    "around 99% test accuracy on MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3. Visualization with TensorBoard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   **TensorBoard**: visualization toolkit shipped with TensorFlow\n",
    "-   Extremely useful - other deep learning frameworks now offer plugins\n",
    "-   Requires log files:\n",
    "    -   for low level interfaces, use `tf.summary.FileWriter` and\n",
    "        `tf.summary.trace_export()`\n",
    "    -   for Keras, use `tf.keras.callbacks.TensorBoard()` callback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Try it out\n",
    "\n",
    "-   Train the LeNet5 model from above and pass a TensorBoard callback.\n",
    "-   Open tensorboard from command line:\n",
    "    `tensorboard --logdir=/path/to/logs`\n",
    "-   Open link `localhost:6006` in browser."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Many different deep learning frameworks, choose what works for your\n",
    "    problem\n",
    "    -   e.g., Keras for standard components\n",
    "    -   e.g., TensorFlow’s low-level interfaces for more detailed\n",
    "        control\n",
    "    -   e.g., PyTorch for dynamic graphs"
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "rise": {
   "auto_select": "code",
   "theme": "solarized"
  }
 }
}

