{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best Practices and Common Pitfalls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%tensorflow_version 2.x\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import Model, Sequential\n",
    "from tensorflow.keras.layers import Conv2D, Flatten, Input, Dense\n",
    "\n",
    "from tensorflow.keras import utils\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Identifying overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Underfitting: model does not capture statistics of training data\n",
    "    -   high training error\n",
    "-   Overfitting: model memorizes idiosyncracies of training data\n",
    "    -   low training error\n",
    "    -   model does not generalize\n",
    "-   Monitor performance on held-out **validation set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from utils import mnist_imgs, mnist_lbls, mnist_convnet\n",
    "\n",
    "# By using `validation_split`, we can automatically reserve a portion of the\n",
    "# training set to monitor validation accuracy\n",
    "model = mnist_convnet()\n",
    "model.fit(mnist_imgs, mnist_lbls, epochs=5, batch_size=64, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "How to prevent overfitting:\n",
    "\n",
    "-   early stopping: stop training once validation accuracy decreases\n",
    "-   regularization: enforce additional restrictions on the network\n",
    "-   use more data, e.g., via data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "cb = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', # stop as soon as validation accuracy decreases\n",
    "    min_delta=0.001, # a decrease of smaller than 0.001 will be ignored\n",
    "    patience=0, # stop as soon as this occurs and do not wait for another epoch\n",
    ")\n",
    "\n",
    "model.fit(mnist_imgs, mnist_lbls, epochs=5, batch_size=64,\n",
    "    validation_split=0.1, callbacks=[cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Appropriate Loss Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   The loss function (or objective) the can have a drastic influence on\n",
    "    the learning process.\n",
    "-   Many loss functions expect a specific input format.\n",
    "-   With wrong loss, code may compile without warning, but produce\n",
    "    nonsensical results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# transform MNIST into dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices((mnist_imgs, mnist_lbls))\n",
    "\n",
    "batched_dataset = dataset.batch(32).repeat(3).shuffle(buffer_size=100000).prefetch(buffer_size=10000)\n",
    "\n",
    "def build_model(input_shape=(28, 28, 1), num_classes=10):\n",
    "    \"\"\"\n",
    "    Build a simple conv net for image classification\n",
    "    :return: the compiled model\n",
    "    \"\"\"\n",
    "    # input is 28 x 28 x 1\n",
    "    inputs = Input(shape=input_shape, name='inputs')\n",
    "    x = Conv2D(filters=64, kernel_size=(3,3), activation='relu', name='dense_1')(inputs)\n",
    "    x = Conv2D(filters=64, kernel_size=(3,3), activation='relu', name='dense_2')(x)\n",
    "    x = Flatten()(x)\n",
    "    outputs = Dense(num_classes, activation='softmax', name='predictions')(x)\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['categorical_accuracy'])\n",
    "    return model\n",
    "  \n",
    "model = build_model()\n",
    "model.fit(batched_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "So what’s the problem? We forgot the transform the label to categorical\n",
    "labels by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import utils\n",
    "mnist_lbls = utils.to_categorical(mnist_lbls, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even with integer labels the computation can by carried out but don’t\n",
    "make sense at all. We could also change the loss function of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['sparse_categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Training vs. testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Some layers change behavior between training and testing\n",
    "-   Example: `Dropout`\n",
    "    -   drops activations and rescales inputs during training\n",
    "    -   passes input through during testing\n",
    "-   Example: `BatchNormalization`\n",
    "    -   uses mean and variance of current batch during training\n",
    "    -   uses learned mean and variance during testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example 1: Training on MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Convolutional network trained on fashionMNIST, using Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = tf.keras.models.load_model(\"../models/model_fmnist.h5\")\n",
    "\n",
    "(train_imgs, train_lbls), (test_imgs, test_lbls) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "train_imgs = train_imgs.reshape(-1, 28, 28, 1) / 255.0\n",
    "test_imgs = test_imgs.reshape(-1, 28, 28, 1) / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "-   When using Keras functions like `.fit()`, `.evaluate()` and\n",
    "    `.predict()`, the model sets the correct mode automatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.evaluate(test_imgs, test_lbls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(test_lbls == tf.argmax(net(test_imgs, training=True), axis=1)).numpy().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(test_lbls == tf.argmax(net(test_imgs, training=False), axis=1)).numpy().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example 2: A custom layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: Implement a fully connected layer that performs dropout on its\n",
    "weight matrix during training. Ensure that the dropout is only applied\n",
    "during training.\n",
    "\n",
    "Subclass\n",
    "[`tf.keras.layer`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.python.keras.utils import tf_utils\n",
    "\n",
    "class DropWeight(Layer):\n",
    "    def __init__(self, units, rate, activation=tf.nn.relu, **kwargs):\n",
    "        super(DropWeight, self).__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.rate = rate\n",
    "        self.activation = activation\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.w = self.add_weight(shape=(input_shape[-1], self.units),\n",
    "            initializer='random_normal', trainable=True)\n",
    "        self.b = self.add_weight(shape=(self.units,),\n",
    "            initializer='random_normal', trainable=True)\n",
    "        \n",
    "    def call(self, x, training=None):\n",
    "        if training is None:\n",
    "            new_w = self.w\n",
    "        elif training is False:\n",
    "            new_w = self.w\n",
    "        else:\n",
    "            new_w = tf.nn.dropout(self.w, self.rate)\n",
    "        return(self.activation(tf.matmul(x, new_w) + self.b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let’s try out this layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Input, Flatten, Dense\n",
    "\n",
    "net = Sequential([\n",
    "    Input(shape=(28, 28, 1)),\n",
    "    Flatten(),\n",
    "    DropWeight(1024, rate=0.2),\n",
    "    DropWeight(1024, rate=0.2),\n",
    "    Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.compile(optimizer='Adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "net.fit(train_imgs, train_lbls, batch_size=64, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Pitfalls in Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Numerical Instabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Policy Gradient Methods we optimize\n",
    "$\\mathbb{E}_\\pi[log \\pi (a|s) R]$, which can lead to numerical\n",
    "instabilities. We can avoid some of them by introducing a rectified log:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rectified_log(x):\n",
    "    return tf.math.log(tf.maximum(1e-6, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In Q-Learning we use the Q-values to define the policy by computing the\n",
    "softmax as\n",
    "\n",
    "$$p(a|s) = \\frac{\\exp(Q(s,a))}{\\exp(\\sum_{a'} Q(s,a')))}$$\n",
    "\n",
    "which can lead to numerical problems if the Q values are large. We can\n",
    "avoid this by subtracting the largest Q value before applying $\\exp$:\n",
    "\n",
    "$$p(a|s) = \\frac{\\exp(Q(s,a) - max(Q(s, \\cdot))}{\\exp(\\sum_{a'} Q(s,a') - max(Q(s, \\cdot)))}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Convergence Issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Initiliazation\n",
    "\n",
    "Try to initialize the weights of the last layer very close to zero,\n",
    "resulting in a policy with high entropy, i.e. uniformly distributed,\n",
    "which helps with exploration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Always Try Multiple Seeds\n",
    "\n",
    "Initialization is very important and can doom your agent right from the\n",
    "beginning. We can try to avoid this trying several random seeds, e.g. by\n",
    "running multiple instances of the agent in parallel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### MaxEnt Reinforcement Learning\n",
    "\n",
    "We can enforce exploration by introducing a bonus for a high entropy\n",
    "policy:\n",
    "\n",
    "$$\\pi^* = \\max_{\\pi} \\mathbb{E}_{\\pi}\\left[\\sum_{t=0}^T r(a_t, s_t) + \\beta H(\\pi)\\right] = \\mathbb{E}_{\\pi}\\left[\\sum_{t=0}^T r(a_t, s_t) - \\beta \\log(\\pi(a_t|s_t))\\right]$$\n",
    "\n",
    "which we can also formulate as penalizes the divergence from a (fixed)\n",
    "policy $\\pi_0$:\n",
    "\n",
    "$$\\pi^* = \\max_{\\pi} \\mathbb{E}_{\\pi}\\left[\\sum_{t=0}^T r(a_t, s_t) - \\beta DKL(\\pi||\\pi_0)\\right],$$\n",
    "\n",
    "where $DKL$ is the Kullback-Leibler Divergence between the probability\n",
    "distributions $p$ and $q$ defined as\n",
    "$$DKL(p|q) = \\sum_x p(x) \\log \\frac{p(x)}{q(x)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### $\\epsilon$ -greedy Exploration\n",
    "\n",
    "Instead of picking an action according to the current policy we can\n",
    "instead encourage exploration by sampling a random action with\n",
    "$\\epsilon$ probability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def epsilon_greedy(q_values, epsilon, num_actions, beta):\n",
    "    u = np.random.uniform()\n",
    "    p = np.ones(shape=num_actions) / num_actions\n",
    "    if u <= epsilon:\n",
    "        a = np.random.randint(low=0, high=num_actions)\n",
    "    else:\n",
    "        max_q = np.max(q_values)\n",
    "        centered_q_values = q_values - max_q\n",
    "        exp_q = np.exp(beta*centered_q_values) \n",
    "        p = exp_q / np.sum(exp_q)\n",
    "        a = np.random.choice(a=num_actions, p=p)\n",
    "    return a, p\n",
    "\n",
    "q_values = np.array([1.0, 1.2, 1.5])\n",
    "epsilon = 0.0\n",
    "\n",
    "a, p = epsilon_greedy(q_values, epsilon, 3, 0.10)\n",
    "print(\"Sampled Action:\", a)\n",
    "print(\"Probability Distribution:\", p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Policy Diagnostic\n",
    "\n",
    "-   Early drop in policy entropy usually means no learning\n",
    "-   High Entropy means that the agent is focusing on a few/a single\n",
    "    action\n",
    "    -   No exploration\n",
    "-   Compute $DKL\\left[\\pi_{old}(·|s)||\\pi_{new}(·|s)\\right]$\n",
    "    -   KL spike means drastic loss of performance\n",
    "    -   No learning progress might mean steps are too large"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Value Function Diagnostic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several papers propose to use the “Huber Loss” instead of Mean Squared\n",
    "Error, which is less sensitive to outliers:\n",
    "\n",
    "$$L_\\delta(y, f(x)) = \\begin{cases}\n",
    " \\frac{1}{2}(y - f(x))^2                   & \\textrm{for } |y - f(x)| \\le \\delta, \\\\\n",
    " \\delta\\, |y - f(x)| - \\frac{1}{2}\\delta^2 & \\textrm{otherwise.}\n",
    "\\end{cases}$$ This function is quadratic for small values of a, and\n",
    "linear for large values, with equal values and slopes of the different\n",
    "sections at the two points where $|a| = \\delta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In Actor-Critic Models we fit a Value function to learn the cumulative\n",
    "reward for a certain state $s$ denoted as $V(s)$. To see if the nework\n",
    "is learning anything one should monitor the explained variance\n",
    "$F(\\theta)$ of your model $\\theta$:\n",
    "\n",
    "$$F(\\theta) = 1 - \\frac{\\mathbb{Var}(y-f(x))}{\\mathbb{Var}(y)},$$\n",
    "\n",
    "where $y$ is the empirical return (i.e. the reward obtained) and $f(x)$\n",
    "is the reward predicted by the Value function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Sample Complexity\n",
    "\n",
    "Usually Deep Reinforcement Learning just takes a lot of time. Model\n",
    "Based RL methods can reduce sample complexity, but that usually means\n",
    "samples drawn from the environment. Training time is often the same or\n",
    "at least comparable to Model Free methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Even with small implementation errors, networks often still train.\n",
    "-   Run as many tests and diagnostics as possible.\n",
    "-   Do not rely on a single metric."
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "rise": {
   "scroll": true,
   "auto_select": "code",
   "theme": "solarized"
  }
 }
}

