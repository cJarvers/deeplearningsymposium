{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Symposium \"Recent Advances in Deep Learning Systems\", Reisensburg/UUlm, 05.11.2019 - 07.11.2019\n",
    "##### Heinke Hihn, Institute for Neural Information Processing, UUlm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Human-level control through deep reinforcement learning\n",
    "## Deep Q-Networks\n",
    "https://daiwk.github.io/assets/dqn.pdf\n",
    "\n",
    "**Abstract**\n",
    ">The theory of reinforcement learning provides a normative account1, deeply rooted in psychological and neuroscientific perspectives on animal behaviour, of how agents may optimize their control of an environment. To use reinforcement learning successfully in situations approaching real-world complexity, however, agents are confronted with a difficult task: they must derive efficient representations of the environment from high-dimensional sensory inputs, and use these to generalize past experience to new situations. Remarkably, humans and other animals seem to solve this problem through a harmonious combination of reinforcement learning and hierarchical sensory processing systems the former evidenced by a wealth of neural data revealing notable parallels between the phasic signals emitted by dopaminergic neurons and temporal difference reinforcement learning algorithms. While reinforcement learning agents have achieved some successes in a variety of domains6 their applicability has previously been limited to domains in which useful features can be handcrafted, or to domains with fully observed, low-dimensional state spaces. Here we use recent advances in training deep neural networks to develop a novel artificial agent, termed a deep Q-network, that can learn successful policies directly from high-dimensional sensory inputs using end-to-end reinforcement learning. We tested this agent on the challenging domain of classic Atari 2600 games. We demonstrate that the deep Q-network agent, receiving only the pixels and the game score as inputs, was able to surpass the performance of all previous algorithms and achieve a level comparable to that of a professional human games tester across a set of 49 games, using the same algorithm, network architecture and hyperparameters. This work bridges the divide between high-dimensional sensory inputs and actions, resulting in the first artificial agent that is capable of learning to excel at a diverse array of challenging tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Q-Learning\n",
    "Q-Learning (Sutton & Barto, 2018) is a Reinforcement Learning Algorithm that aims to learn the future discounted reward in state $s$ while executing action $a$, i.e. the Q-function $Q(s,a)$ is defined as\n",
    "\n",
    "$$Q(s_t, a_t) = \\max_\\pi R_{t+1},$$\n",
    "\n",
    "where $\\pi$ is the agents policy and $R_t$ is the cumulative reward from step $t$ onward:\n",
    "\n",
    "$$R_t = r_t + r_{t+1} + ... + r_{n},$$\n",
    "\n",
    "because the horizon $n$ can be infinte we introduce a discount factor $\\gamma \\in [0,1]$ to make the sum converge:\n",
    "\n",
    "$$R_t =  \\gamma^{t}r_t + \\gamma^{t+1}t_{t+1} + ... + \\gamma^{t+1}t_{n},$$\n",
    "\n",
    "so we can express this as\n",
    "\n",
    "$$R_t=r_t+\\gamma (r_{t+1}+\\gamma (r_{t+2}+…))=r_t+\\gamma R_{t+1}.$$\n",
    "\n",
    "A way to think of this is the following: the further away the reward is the less we value it. An optimal strategy $\\pi^*$ would in this formulation always choose an action that maximizes the cumulative discounted future reward.\n",
    "We can learn the optimal Q-function via the **Bellman Equation**:\n",
    "\n",
    "$$Q(s,a)=r + \\gamma max_{a’}Q(s’,a’)$$\n",
    "\n",
    "It can summarized as folows: the maximum future reward for this state and action is the immediate reward plus **maximum** future reward for the next state.\n",
    "\n",
    "We can learn the Q-function by parametrizing it via a neural network and performing regression on the following regression (Mnih et al., 2015):\n",
    "$$L=\\frac{1}{2}\\left(\\underbrace{r + \\gamma max_{a'}Q(s',a')}_{\\text{target}} - \\underbrace{Q(s,a)}_{\\text{prediction}}\\right)^2$$\n",
    "\n",
    "Basically we can achieve this by iterating the following steps:\n",
    "\n",
    "1. Perform feedforward pass for the current state $s$ to get predicted Q-values\n",
    "2. Perform second feedforward pass for the next state $s'$ and find maximum over $Q(s', \\cdot)$\n",
    "3. Set Q-value target for action a to $r+γmaxa′Q(s′,a′)$ . **Important**: For the remaining actions set the target to the same as originally returned from step 1 (0 error, so no update)\n",
    "4. Update the weights using backpropagation on regression loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experience Replay\n",
    "The most important trick to stabilize learning in DQNs is experience replay. During gameplay all the experience tuples  $(s,a,r,s')$ are stored in a replay memory. During training we now sample a batch of $N$ samples uniformly from the experience memory and use that batch for updating. In this way the samples are not correlated and we can view this as a simple supervised regression problem. The samples used for training are independent and identically distributed (i.i.d.), which helps in training. There are several versions and improvements of experience replays, e.g. prioritized experience replay (Schaul et al., 2015) and hindsight experience replay(Andrychowicz et al., 2017)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target Network\n",
    "We use two deep networks $\\theta^-$ and $\\theta$. We use the first one to retrieve Q values while the second one includes all updates in the training. After a certain amount of updates, we set $\\theta^- \\leftarrow \\theta$. The purpose is to fix the Q-value targets temporarily so we don't have a moving target to chase. In addition, parameter changes do not impact $\\theta^-$ immediately and therefore even the input may not be i.i.d., it will not incorrectly magnify its effect as mentioned before. The Bellmann loss becomes\n",
    "\n",
    "$$L=\\frac{1}{2}\\left(\\underbrace{r + \\gamma max_{a'}Q_{\\theta^-}(s',a')}_{\\text{target}} - \\underbrace{Q_{\\theta}(s,a)}_{\\text{prediction}}\\right)^2,$$\n",
    "\n",
    "where $Q_{\\theta^-}$ is the Q-function given by the target network and $Q_{\\theta}$ the Q-function given by the DQN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References \n",
    "**(Schaul et al., 2015)** Schaul, T., Quan, J., Antonoglou, I., & Silver, D. (2015). Prioritized experience replay. arXiv preprint arXiv:1511.05952.\n",
    "\n",
    "**(Andrychowicz et al., 2017)** Andrychowicz, M., Wolski, F., Ray, A., Schneider, J., Fong, R., Welinder, P., ... & Zaremba, W. (2017). Hindsight experience replay. In Advances in Neural Information Processing Systems (pp. 5048-5058).\n",
    "\n",
    "\n",
    "**(Mnih et al., 2015)** Mnih, Volodymyr, et al. \"Human-level control through deep reinforcement learning.\" Nature 518.7540 (2015): 529.\n",
    "\n",
    "**(Sutton & Barto, 2018)** Sutton, R.S.; Barto, A.G. *Reinforcement learning: An introduction*; MIT press, 2018."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorflow_version 2.x\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import optimizers, losses\n",
    "from tensorflow.keras import Model\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNModel(Model):\n",
    "    def __init__(self):\n",
    "        super(DQNModel, self).__init__()\n",
    "        self.layer1 = Dense(64, activation='relu')\n",
    "        self.layer2 = Dense(64, activation='relu')\n",
    "        self.value = Dense(num_action)\n",
    "\n",
    "    def call(self, state):\n",
    "        layer1 = self.layer1(state)\n",
    "        layer2 = self.layer2(layer1)\n",
    "        value = self.value(layer2)\n",
    "        return value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self):\n",
    "        # hyper parameters\n",
    "        self.lr = 0.001\n",
    "        self.lr2 = 0.001\n",
    "        self.gamma = 0.99\n",
    "\n",
    "        # create model and target model\n",
    "        self.dqn_model = DQNModel()\n",
    "        self.dqn_target = DQNModel()\n",
    "        self.opt = optimizers.Adam(lr=self.lr)\n",
    "\n",
    "        # epsilon-greedy action selection\n",
    "        # with decaying epsilon\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_decay = 0.999\n",
    "        self.epsilon_min = 0.01\n",
    "        self.batch_size = 64\n",
    "        # we start training after we have at least 1000 tuples in the replay memory\n",
    "        self.train_start = 1000\n",
    "        self.state_size = state_size\n",
    "        # replay memory\n",
    "        self.memory = deque(maxlen=2000)\n",
    "\n",
    "    def update_target(self):\n",
    "        \"\"\"\n",
    "        Updates the target network\n",
    "        :return: none\n",
    "        \"\"\"\n",
    "        self.dqn_target.set_weights(self.dqn_model.get_weights())\n",
    "\n",
    "    def get_action(self, state):\n",
    "        \"\"\"\n",
    "        Select an action given a state. Implements epsilon-greedy strategy\n",
    "        :param state: state input\n",
    "        :return: selected action\n",
    "        \"\"\"\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(num_action)\n",
    "        else:\n",
    "            q_value = self.dqn_model(tf.convert_to_tensor(state[None, :], dtype=tf.float32))\n",
    "            return np.argmax(q_value[0])\n",
    "\n",
    "    def append_tuple(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        Append a tuple to experience buffer\n",
    "        :param state: obsvered state\n",
    "        :param action: executed action\n",
    "        :param reward: reward recieved\n",
    "        :param next_state: state reached after exectuting a in s\n",
    "        :param done: whether or not environment reached a terminal state\n",
    "        :return: none\n",
    "        \"\"\"\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Trains the DQN model.\n",
    "        :return: none\n",
    "        \"\"\"\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "        # experience replay\n",
    "        mini_batch = random.sample(self.memory, self.batch_size)\n",
    "\n",
    "        states = np.zeros((self.batch_size, self.state_size))\n",
    "        next_states = np.zeros((self.batch_size, self.state_size))\n",
    "        actions, rewards, dones = [], [], []\n",
    "        # split batch\n",
    "        for i in range(self.batch_size):\n",
    "            states[i] = mini_batch[i][0]\n",
    "            actions.append(mini_batch[i][1])\n",
    "            rewards.append(mini_batch[i][2])\n",
    "            next_states[i] = mini_batch[i][3]\n",
    "            dones.append(mini_batch[i][4])\n",
    "\n",
    "        dqn_variable = self.dqn_model.trainable_variables\n",
    "\n",
    "        # compute gradients and uptadate model parameters\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(dqn_variable)\n",
    "            # get targets\n",
    "            target = self.dqn_model(tf.convert_to_tensor(np.vstack(states), dtype=tf.float32))\n",
    "            target_val = self.dqn_target(tf.convert_to_tensor(np.vstack(next_states), dtype=tf.float32))\n",
    "\n",
    "            target = np.array(target)\n",
    "            target_val = np.array(target_val)\n",
    "\n",
    "            for i in range(self.batch_size):\n",
    "                next_v = np.array(target_val[i]).max()\n",
    "                # q-learning\n",
    "                if dones[i]:\n",
    "                    # if terminal state just take the reward\n",
    "                    target[i][actions[i]] = rewards[i]\n",
    "                else:\n",
    "                    # if not terminal state we also take discounted future reward into account\n",
    "                    target[i][actions[i]] = rewards[i] + self.gamma * next_v\n",
    "\n",
    "            values = self.dqn_model(tf.convert_to_tensor(np.vstack(states), dtype=tf.float32))\n",
    "            error = tf.square(values - target) * 0.5\n",
    "            error = tf.reduce_mean(error)\n",
    "\n",
    "        dqn_grads = tape.gradient(error, dqn_variable)\n",
    "        self.opt.apply_gradients(zip(dqn_grads, dqn_variable))\n",
    "\n",
    "    def run(self, env):\n",
    "\n",
    "        max_ep_len = 500\n",
    "        episodes = 1000\n",
    "\n",
    "        state = env.reset()\n",
    "        state = np.reshape(state, [1, state_size])\n",
    "\n",
    "        for e in range(episodes):\n",
    "            total_reward = 0\n",
    "            for t in range(max_ep_len):\n",
    "                action = self.get_action(state)\n",
    "                next_state, reward, done, info = env.step(action)\n",
    "                next_state = np.reshape(next_state, [1, state_size])\n",
    "\n",
    "                #env.render()\n",
    "\n",
    "                # if \n",
    "                if t == max_ep_len :\n",
    "                    done = True\n",
    "                # \n",
    "                if t < max_ep_len and done :\n",
    "                    reward = -1\n",
    "\n",
    "                total_reward += reward\n",
    "                self.append_tuple(state, action, reward, next_state, done)\n",
    "                \n",
    "                # train if memory has enought tuples\n",
    "                if len(self.memory) >= self.train_start:\n",
    "                    self.train()\n",
    "\n",
    "                total_reward += reward\n",
    "                state = next_state\n",
    "                \n",
    "                if done:\n",
    "                    # if done update target network if necessary\n",
    "                    self.update_target()\n",
    "                    print(\"e : \", e, \" reward : \", total_reward, \" step : \", t)\n",
    "                    env.reset()\n",
    "                    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "num_action = env.action_space.n\n",
    "state_size = env.observation_space.shape[0]\n",
    "\n",
    "DQN = Trainer()\n",
    "DQN.run(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hackathon Project\n",
    "Implement an DQN agent that learns to solve the Pong game (gym.make(\"Pong-v0\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
